{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc6d30d-535b-4e19-91c6-28001905e911",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Structured Query Language, or SQL, is the programming language used with databases, and it is an important skill for any data scientist. In this course, you'll build your SQL skills using BigQuery, a web service that lets you apply SQL to huge datasets.\n",
    "\n",
    "In this lesson, you'll learn the basics of accessing and examining BigQuery datasets. After you have a handle on these basics, we'll come back to build your SQL skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e009c-b265-4f3e-a987-2401af0f7804",
   "metadata": {},
   "source": [
    "## Your first BigQuery commands\n",
    "\n",
    "To use BigQuery, we'll import the Python package below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef705d62-1063-45f8-9dc3-1c0862825b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-bigquery  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3ca7c7-cc96-4cf4-94c1-bba9c468fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\data_science\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218670f2-a44d-4a4d-9d25-639ef0efc258",
   "metadata": {},
   "source": [
    "The first step in the workflow is to create a Client object. As you'll soon see, this Client object will play a central role in retrieving information from BigQuery datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f79cc1-9430-4631-adeb-137d0c1df16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the full absolute path of your downloaded key \n",
    "# get the key form google console to be able to access bigquery which is hosted my google\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"E:\\\\c_extend\\\\Documents\\\\egerdrive-7433adb919ad.json\"\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/mwaki/Documents/Documents/Credentials/egerdrive-7433adb919ad.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ca10e7-ea55-478e-a7ac-1c34fe11ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395307d-242a-4e60-98ef-0e2a5117ed06",
   "metadata": {},
   "source": [
    "We'll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news.\n",
    "\n",
    "In BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project. To access the dataset,\n",
    "\n",
    "    We begin by constructing a reference to the dataset with the dataset() method.\n",
    "    Next, we use the get_dataset() method, along with the reference we just constructed, to fetch the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1f2757-aa31-4640-a5d0-7e750aa8c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(\"hacker_news\",project=\"bigquery-public-data\")\n",
    "# Api request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cc8f9-88f4-498f-a878-9d9e398b736e",
   "metadata": {},
   "source": [
    "Every dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.\n",
    "\n",
    "We use the list_tables() method to list the tables in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6131b880-c114-4fab-a082-8d2f1905b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full\n"
     ]
    }
   ],
   "source": [
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b670d9c-1838-4769-be04-35b59fdd8cb5",
   "metadata": {},
   "source": [
    "Similar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in the hacker_news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7040750-0610-428b-bb6f-e5a51bf1fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a reference to the full talbe\n",
    "table_ref = dataset_ref.table(\"full\")\n",
    "# api request - fetch the table\n",
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef44148-36aa-45bf-8131-084f791a7624",
   "metadata": {},
   "source": [
    "Similar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in the hacker_news dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455e529-b9e9-4958-b34b-19dce43be2da",
   "metadata": {},
   "source": [
    "## Table schema\n",
    "\n",
    "The structure of a table is called its *schema*. We need to understand a table's schema to effectively pull out the data we want.\n",
    "\n",
    "In this example, we'll investigate the full table that we fetched above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68514658-1cb6-4c3c-adc1-3a10a4752a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('title', 'STRING', 'NULLABLE', None, 'Story title', (), None),\n",
       " SchemaField('url', 'STRING', 'NULLABLE', None, 'Story url', (), None),\n",
       " SchemaField('text', 'STRING', 'NULLABLE', None, 'Story or comment text', (), None),\n",
       " SchemaField('dead', 'BOOLEAN', 'NULLABLE', None, 'Is dead?', (), None),\n",
       " SchemaField('by', 'STRING', 'NULLABLE', None, \"The username of the item's author.\", (), None),\n",
       " SchemaField('score', 'INTEGER', 'NULLABLE', None, 'Story score', (), None),\n",
       " SchemaField('time', 'INTEGER', 'NULLABLE', None, 'Unix time', (), None),\n",
       " SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', None, 'Timestamp for the unix time', (), None),\n",
       " SchemaField('type', 'STRING', 'NULLABLE', None, 'type of details (comment comment_ranking poll story job pollopt)', (), None),\n",
       " SchemaField('id', 'INTEGER', 'NULLABLE', None, \"The item's unique id.\", (), None),\n",
       " SchemaField('parent', 'INTEGER', 'NULLABLE', None, 'Parent comment ID', (), None),\n",
       " SchemaField('descendants', 'INTEGER', 'NULLABLE', None, 'Number of story or poll descendants', (), None),\n",
       " SchemaField('ranking', 'INTEGER', 'NULLABLE', None, 'Comment ranking', (), None),\n",
       " SchemaField('deleted', 'BOOLEAN', 'NULLABLE', None, 'Is deleted?', (), None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0ad15-1bbb-4958-a72f-1ac55fd8e3b5",
   "metadata": {},
   "source": [
    "Each SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:\n",
    "\n",
    "    The name of the column\n",
    "    The field type (or datatype) in the column\n",
    "    The mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\n",
    "    A description of the data in that column\n",
    "\n",
    "The first field has the SchemaField:\n",
    "\n",
    "SchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())\n",
    "\n",
    "This tells us:\n",
    "\n",
    "    the field (or column) is called by,\n",
    "    the data in this field is strings,\n",
    "    NULL values are allowed, and\n",
    "    it contains the usernames corresponding to each item's author.\n",
    "\n",
    "We can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. (Sometimes databases have outdated descriptions, so it's good to check.) This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a8a8ae-63c8-4226-9ee8-aecbd27aa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install db-dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39020941-9814-4991-a4a3-f65906d15d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please install the 'pandas' package to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\data_science\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:39\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     41\u001b[39m     pandas_import_exception = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Preview the first five lines of the \"full\" table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m client.list_rows(table, max_results=\u001b[32m5\u001b[39m).to_dataframe()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\data_science\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2553\u001b[39m, in \u001b[36mRowIterator.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   2318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2320\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2339\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   2340\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a pandas DataFrame by loading all pages of a query.\u001b[39;00m\n\u001b[32m   2342\u001b[39m \n\u001b[32m   2343\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2551\u001b[39m \n\u001b[32m   2552\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     _pandas_helpers.verify_pandas_imports()\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m geography_as_object \u001b[38;5;129;01mand\u001b[39;00m shapely \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2556\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_SHAPELY_ERROR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\data_science\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:1120\u001b[39m, in \u001b[36mverify_pandas_imports\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mverify_pandas_imports\u001b[39m():\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pandas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_PANDAS_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas_import_exception\u001b[39;00m\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m db_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_DB_TYPES_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes_import_exception\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Please install the 'pandas' package to use this function."
     ]
    }
   ],
   "source": [
    "# Preview the first five lines of the \"full\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf56ca1-cdfd-40d1-9d86-6124bb1b09ba",
   "metadata": {},
   "source": [
    "The list_rows() method will also let us look at just the information in a specific column. If we want to see the first five entries in the by column, for example, we can do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e11cc-ac2a-49e2-914f-45a3297420cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five entries in the \"by\" column of the \"full\" table\n",
    "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef7dee-a525-4ca7-958f-57eee1acf0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
